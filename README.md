# ETL Mini Pipeline â€” Task 14

## ğŸ“Œ Project Overview

This project implements a mini ETL (Extract â†’ Transform â†’ Load) pipeline using Python.
The goal is to simulate a real-world data analytics workflow by cleaning raw data, transforming it, and storing structured outputs.

---

## ğŸ›  Tools & Technologies

* Python
* Pandas
* SQLite
* Google Colab / Jupyter Notebook
* CSV files

---

## ğŸ“‚ Project Folder Structure

```
task14_etl/
â”‚
â”œâ”€â”€ raw/
â”‚   â””â”€â”€ raw_data.csv
â”‚
â”œâ”€â”€ processed/
â”‚   â””â”€â”€ processed_data.csv
â”‚
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ customers.csv
â”‚   â”œâ”€â”€ orders.csv
â”‚   â””â”€â”€ products.csv
â”‚
â”œâ”€â”€ database.sqlite
â””â”€â”€ task14_etl.ipynb
```

---

## ğŸ”„ ETL Workflow Steps

### 1. Extract

* Loaded dataset from CSV source
* Saved raw copy in `raw/` folder

### 2. Transform

* Removed duplicates
* Cleaned missing values
* Standardized column names
* Created derived columns:

  * Profit margin
  * High value customer flag

### 3. Load

* Split dataset into:

  * Customers table
  * Orders table
  * Products table
* Exported processed CSV files
* Stored structured tables in SQLite database

---

## âœ… Validation

* Verified row counts before and after transformation
* Ensured clean and structured outputs

---

## ğŸ¯ Final Outcome

This project demonstrates how ETL pipelines are built in analytics workflows.
It improves data cleaning, transformation, and database handling skills.

---

## ğŸš€ How to Run

1. Open `task14_etl.ipynb` in Google Colab or Jupyter
2. Run all cells step-by-step
3. Generated files will appear in project folders
4. Upload project to GitHub

---

## ğŸ“ Dataset Source

Superstore retail dataset (CSV)

---

## ğŸ‘¨â€ğŸ’» Author
MANOJRAJ G

Data Analyst Internship â€” Task 14
